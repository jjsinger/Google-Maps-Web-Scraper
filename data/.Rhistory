#a
A3i <- solve(A3)
#b
B3i <- solve(B3)
#c
solve(A3%*%B3)
#d
A3i%*%B3i
#a
A3i <- solve(A3)
#b
B3i <- solve(B3)
#c
solve(A3%*%B3)
#d
B3i%*%A3i
A6 <- matrix(c(4,11,3,2), 2,2)
B6 <- matrix(c(2,5,1,6, 8, 333, 1, 10, 4,1,7,423,0,0,0,0), 4,4)
C6 <- C3
A6 <- matrix(c(4,11,3,2), 2,2)
B6 <- matrix(c(2,5,1,6, 8, 333, 1, 10, 4,1,7,423,0,0,0,0), 4,4)
C6 <- C
D6 <- matrix(c(4,0,0,2), 2,2)
A6 <- matrix(c(4,11,3,2), 2,2)
det(A6)
#det(A6) is -27
B6 <- matrix(c(2,5,1,6, 8, 333, 1, 10, 4,1,7,423,0,0,0,0), 4,4)
#det(B6) is 7
C6 <- C
det(C6)
#det(C6) is -1
D6 <- matrix(c(4,0,0,2), 2,2)
det(D6)
#det(D6)
A6 <- matrix(c(4,11,3,2), 2,2)
det(A6)
#det(A6) is -27
B6 <- matrix(c(2,5,1,6, 8, 333, 1, 10, 4,1,7,423,0,0,0,0), 4,4)
det(B6)
#det(B6) is 7
C6 <- C
det(C6)
#det(C6) is -1
D6 <- matrix(c(4,0,0,2), 2,2)
det(D6)
#det(D6)
A6 <- matrix(c(4,11,3,2), 2,2)
det(A6)
#det(A6) is -27
B6 <- matrix(c(2,5,1,6, 8, 333, 1, 10, 4,1,7,423,0,0,0,0), 4,4)
det(B6)
#det(B6) is 0
C6 <- C
det(C6)
#det(C6) is -1
D6 <- matrix(c(4,0,0,2), 2,2)
det(D6)
solve(D6) == abs(D6)
#det(D6) is 8
A6 <- matrix(c(4,11,3,2), 2,2)
det(A6)
#det(A6) is -27
B6 <- matrix(c(2,5,1,6, 8, 333, 1, 10, 4,1,7,423,0,0,0,0), 4,4)
det(B6)
#det(B6) is 0
C6 <- C
det(C6)
#det(C6) is -1
D6 <- matrix(c(4,0,0,2), 2,2)
det(D6)
t(D6) == abs(D6)
#det(D6) is 8
a11 <- matrix(c(7,-3, 2,4,5,0), 2,3)
a12 <- matrix(c(8,2),2,1)
a21 <- matrix(c(9,3,3,1,6,2), 2,3)
a22 <- matrix(c(5,1), 2,1)
A7 <- matrix(c(7,-3,2,4,3,1,5,0,6,2,9,3,,8,2,5,1))
A7 <- matrix(c(7,-3,2,4,3,1,5,0,6,2,9,3,,8,2,5,1), 4,4)
A7 <- matrix(c(7,-3,9,3,2,4,3,1,5,0,6,2,8,2,5,1), 4,4)
a11 <- matrix(c(7,-3, 2,4,5,0), 2,3)
a12 <- matrix(c(8,2),2,1)
a21 <- matrix(c(9,3,3,1,6,2), 2,3)
a22 <- matrix(c(5,1), 2,1)
A7
A7 <- matrix(c(7,-3,9,3,2,4,3,1,5,0,6,2,8,2,5,1), 4,4)
a11 <- A7[1:2,1:3]
a12 <- A7[1:2,4]
a21 <- A7[3:4,1:3]
a22 <- A7[3:4,4]
A7 <- matrix(c(7,-3,9,3,2,4,3,1,5,0,6,2,8,2,5,1), 4,4)
a11 <- A7[1:2,1:3]
a12 <- A7[1:2,4]
a21 <- A7[3:4,1:3]
a22 <- A7[3:4,4]
a11
t(c6)
t(C6)
C6
A7 <- matrix(c(7,-3,9,3,2,4,3,1,5,0,6,2,8,2,5,1), 4,4)
a11 <- A7[1:2,1:3]
a12 <- A7[1:2,4]
a21 <- A7[3:4,1:3]
a22 <- A7[3:4,4]
a11
a12
a21
a22
library(dplyr)
library(ggplot2)
library(gridExtra)
library(GGally)
install.packages('dplyr')
install.packages('ggplot')
install.packages('GGally')
install.packages('ggplot2')
#read in birthweights data
babies <- read.table("BirthWeights.txt", header = TRUE)
glimpse(babies)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(GGally)
#read in birthweights data
babies <- read.table("BirthWeights.txt", header = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Cross Validation using K-fold
n.cv <- 100
cvg <- rep(NA, n.cv)
n.test <- round(0.25*nrow(food))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(car)
library(MASS)
library(lmtest)
library(multcomp)
library(lubridate)
library(nlme)
library(data.table)
library(RCurl)
library(knitr)
source('predictgls.R')
food <- read.table('https://mheaton.byu.edu/Courses/Stat469/Topics/1%20-%20Independence/2%20-%20Diagonal/HWCaseStudy/Data/FoodExpenses.txt', header = TRUE)
head(food)
#length(unique(food$Income)) #13 different unique Income variables
#unique(food$EatingOut)
food_scat <- ggplot(food, aes(Income, EatingOut)) + geom_point() + geom_smooth(method=lm, coor='red')
food_scat + labs(title = 'Scatter Plot of EatingOut by Income')
inc_hist <- ggplot() + geom_histogram(aes(food$Income))
inc_hist + labs(title = 'Histogram of Income')
inc_log <- log(food$Income)
eo_log <- log(food$EatingOut)
log_data <- data.frame(inc=inc_log, eo=eo_log)
log_scat <- ggplot(log_data, aes(inc_log, eo_log)) + geom_point() + geom_smooth(method=lm)
log_scat + labs(title = 'Scatter Plot of Log of EatingOut by Income')
#Homoskedastic Linear Model
fit <- lm(EatingOut~Income, food)
summary(fit)
fit_resids <- fit$residuals
#Linearity
food_scat <- ggplot() + geom_point(aes(food$Income, food$EatingOut)) + geom_smooth()
food_scat + labs(title = "EatingOut vs Income Scatter Plot")
#Independence
#We will assume independence because one persons income is not dependent on another's income.
#Normality
food_hist <- ggplot()+geom_histogram(mapping=aes(fit_resids))
food_hist + labs(title = "Histogram of Residuals (Food Expenditures)")
#Equal Variances
food_res <- resid(fit, type = "pearson")
food_fitted <- fit$fitted
res_plot <- ggplot() + geom_point(aes(food_fitted, fit_resids))
res_plot + labs(title = "Fitted vs Residuals")
#Heteroskedastic linear regression model
#y ~ N(XB, sigma^2%*%D(theta))
#Our model uses a exponential variance function method to fit the heteroskedastic model
#Heteroskedastic Model
fit_gls <- gls(model=EatingOut~Income, data=food, weights=varExp(form=~Income))
summary(fit_gls)
#TESTING LINEAR ASSUMPTIONS FOR HETEROSKEDASTIC MODEL
#Residual and Fitted values
resids_gls <- resid(fit_gls, type = "pearson")
fitted_gls <- fit_gls$fitted
#Betahat
fit_gls$coefficients
#Sigma
fit_gls$sigma
#Thetahat
coef(fit_gls$modelStruct, unconstrained = FALSE)
#Linearity
food_scat <- ggplot() + geom_point(aes(food$Income, food$EatingOut)) + geom_smooth()
food_scat + labs(title = "EatingOut vs Income Scatter Plot")
#Independence
#We will assume independence because one persons income is not dependent on another's income.
#Normality
food_hist <- ggplot()+geom_histogram(mapping=aes(resids_gls))
food_hist + labs(title = "Histogram of Residuals (Heteroskedastic Model)")
#Equal Variances
res_plot <- ggplot() + geom_point(aes(fitted_gls, resids_gls))
res_plot + labs(title = "Fitted vs Residuals (Heteroskedastic Model)")
# Cross Validation using K-fold
n.cv <- 100
cvg <- rep(NA, n.cv)
n.test <- round(0.25*nrow(food))
rpmse <- rep(NA, n.cv)
width <- rep(NA, n.cv)
bias <- rep(NA, n.cv)
for(cv in 1:n.cv){
## Split training and test
test.obs <- sample(1:nrow(food), n.test)
test.set <- food[test.obs,]
train.set <- food[-test.obs,]
## Fit GLS model to trainset
train.gls <- gls(EatingOut~Income, data = train.set,
weights=varExp(form=~Income))
## Predict test set
test.preds <- predictgls(train.gls, newdframe=test.set)
## Calculate pre and inter (exp to get rid of logs)
EOpred <- test.preds$Prediction
EO.low <- test.preds$Prediction - qt(1-0.025/2, df = nrow(train.set) - length(coef(train.gls))*test.preds$SE.pred)
EO.up <- test.preds$Prediction +  qt(1-0.025/2, df = nrow(train.set) - length(coef(train.gls))*test.preds$SE.pred)
## Calculate diagnostics: bias, RPMSE, Coverage, and Width
#bias[cv] <- (EOpred - test.set$EatingOut) %>% mean()
rpmse[cv] <- (EOpred - test.set$EatingOut)^2 %>% mean() %>% sqrt()
cvg[cv] <- mean((EO.low < test.set$EatingOut) & (EO.up > test.set$EatingOut))
#width[cv] <- (EO.up - EO.low) %>% mean()
}
#mean(bias)
mean(rpmse)
mean(cvg)
#mean(width)
ggplot(test.preds) + geom_point(aes(test.preds$Income, test.preds$EatingOut)) + geom_smooth(method=lm)
ggplot(test.preds) + geom_point(aes(Income, EatingOut))
ggplot(test.preds) + geom_point(aes(Income, EatingOut)) + geom_smooth(method=lm)
ggplot(test.preds, aes(Income, EatingOut)) + geom_point() + geom_smooth(method=lm)
ggplot(train.preds, aes(Income, EatingOut)) + geom_point() + geom_smooth(method=lm)
ggplot(train.gls, aes(Income, EatingOut)) + geom_point() + geom_smooth(method=lm)
# Cross Validation using K-fold
n.cv <- 100
cvg <- rep(NA, n.cv)
n.test <- round(0.25*nrow(food))
rpmse <- rep(NA, n.cv)
width <- rep(NA, n.cv)
bias <- rep(NA, n.cv)
for(cv in 1:n.cv){
## Split training and test
test.obs <- sample(1:nrow(food), n.test)
test.set <- food[test.obs,]
train.set <- food[-test.obs,]
## Fit GLS model to trainset
train.gls <- gls(EatingOut~Income, data = train.set,
weights=varExp(form=~Income))
## Predict test set
test.preds <- predictgls(train.gls, newdframe=test.set)
## Calculate pre and inter (exp to get rid of logs)
EOpred <- test.preds$Prediction
EO.low <- test.preds$Prediction - qt(1-0.025/2, df = nrow(train.set) - length(coef(train.gls))*test.preds$SE.pred)
EO.up <- test.preds$Prediction +  qt(1-0.025/2, df = nrow(train.set) - length(coef(train.gls))*test.preds$SE.pred)
## Calculate diagnostics: bias, RPMSE, Coverage, and Width
#bias[cv] <- (EOpred - test.set$EatingOut) %>% mean()
rpmse[cv] <- (EOpred - test.set$EatingOut)^2 %>% mean() %>% sqrt()
cvg[cv] <- mean((EO.low < test.set$EatingOut) & (EO.up > test.set$EatingOut))
#width[cv] <- (EO.up - EO.low) %>% mean()
}
#mean(bias)
mean(rpmse)
mean(cvg)
#mean(width)
ggplot(train.gls, aes(Income, EatingOut)) + geom_point() + geom_smooth(method=lm)
ggplot(test.preds, aes(Income, EatingOut)) + geom_point() + geom_smooth(method=lm)
scat_preds <- ggplot(test.preds, aes(Income, EatingOut)) + geom_point() + geom_smooth(method=lm)
scat_preds + labs(title="Income vs EatingOut Predictions")
knitr::opts_chunk$set(echo = TRUE)
pedagogy <- read.csv('https://mheaton.byu.edu/Courses/Stat469/Topics/1%20-%20Independence/3%20-%20Project/Data/ClassAssessment.txt', header=TRUE)
pedagogy <- read.csv('https://mheaton.byu.edu/Courses/Stat469/Topics/1%20-%20Independence/3%20-%20Project/Data/ClassAssessment.txt', header=TRUE)
head(pedagogy)
#Method 1: MonteCarlo
samp <- rnorm(5000,-10,4)
mean(samp < -13)
#Method 2
pnorm(-13,-10,4)
"Exact Answer"
y <- 19
a.post <- 5.8 + y
b.post <- 7.1 + (42-y)
qbeta(c(.05,.95), a.post, b.post)
#MONTECARLO
r.beta <- rbeta(5000,a.post,b.post)
quantile(r.beta, c(.05,.95))
#expected value is n*theta
exp.binom <- 109*.63
exp.binom
#MONTE CARLO
b.monte <- rbinom(5000,109,.63)
mean(b.monte)
#PRIOR DENSITY CURVE
pr.beta.will <- rbeta(100000, 50, 120)
pr.beta.dem <- rbeta(100000, 50, 120)
pr.diff <- pr.beta.will - pr.beta.dem
a.will <- 50 + 2654
b.will <- 120 + (7706-2654)
a.dem <- 50 + 2214
b.dem <- 120 + (6821-2214)
beta.will <- rbeta(100000, a.will, b.will)
beta.dem <- rbeta(100000, a.dem, b.dem)
diff <- beta.will - beta.dem
mean(diff >= 0)
# Williams was better than demaggio because there were more times where the difference was positive
quantile(diff, c(.025,.975))
plot(density(diff), main="Posterior & Prior Density Plots for ", ylab="Frequency", xlab="Probabilities", col="black", lwd=3)
lines(density(pr.diff), col="gray", lwd=3)
legend("topright", c("Prior", "Posterior"), col=c("gray", "black"), lwd=3)
#PRIOR DENSITY CURVE
pr.beta.will <- rbeta(100000, 50, 120)
mean(pr.diff)
mean(pr.diff>0)
#PRIOR DENSITY CURVE
pr.beta.will <- rbeta(100000, 50, 120)
pr.beta.dem <- rbeta(100000, 50, 120)
pr.diff <- pr.beta.will - pr.beta.dem
mean(pr.diff>0)
a.will <- 50 + 2654
b.will <- 120 + (7706-2654)
a.dem <- 50 + 2214
b.dem <- 120 + (6821-2214)
beta.will <- rbeta(100000, a.will, b.will)
beta.dem <- rbeta(100000, a.dem, b.dem)
diff <- beta.will - beta.dem
mean(diff >= 0)
# Williams was better than demaggio because there were more times where the difference was positive
glimpse(pedagogy)
pedagogy <- read.table('https://mheaton.byu.edu/Courses/Stat469/Topics/1%20-%20Independence/3%20-%20Project/Data/ClassAssessment.txt', header=TRUE)
head(pedagogy)
glimpse(pedagogy)
ggplot(pedagogy) + geom_point(aes(pedagogy$Semester, pedagogy$Exam1))
ggplot(pedagogy) + geom_point(aes(pedagogy$Semester, Exam1, Exam2, Exam3))
ggplot(pedagogy) + geom_point(aes(pedagogy$Semester, Exam1, Exam2, Exam3))
ggplot(pedagogy) + geom_point(aes(pedagogy$NStudents, Exam1, Exam2, Exam3))
ggplot(pedagogy) + geom_line(aes(pedagogy$NStudents, Exam1, Exam2, Exam3))
ggplot(pedagogy) + geom_line(aes(pedagogy$NStudents, Exam1))
ggplot(pedagogy) + geom_point(aes(pedagogy$NStudents, Exam1))
ggplot(pedagogy) + geom_point(aes(pedagogy$NStudents, Exam1, group_by = Semester))
ggplot(pedagogy) + geom_point(aes(pedagogy$NStudents, Exam1, group = Semester))
ggplot(pedagogy) + geom_point(aes(pedagogy$NStudents, Exam1, fill = Semester))
ggplot(pedagogy) + geom_point(aes(pedagogy$NStudents, Exam1, color = Semester))
library(reshape2)
library(dplyr)
library(ggplot2)
library(car)
library(MASS)
library(lmtest)
library(multcomp)
library(lubridate)
library(nlme)
library(data.table)
library(RCurl)
library(knitr)
pedagogy <- read.table('https://mheaton.byu.edu/Courses/Stat469/Topics/1%20-%20Independence/3%20-%20Project/Data/ClassAssessment.txt', header=TRUE)
head(pedagogy)
height <- c(1,1,1,1,35)
height(length(height))
height[length(height)]
dnorm(1.96)
rownum(height)
type(1)
type(height)
numeric()
is.logical(1)
print.objects()
getws()
x <- 1
getws()
ls()
list.objects()
list(height)
leng(1,2,2,2,35)
c(1,2,2,2,35)
leng <- c(1,2,2,2,35)
list(height, leng)
unlist(list(height,leng))
Renviron.site
Rdefaults.site
Rstatus.site
Rstatus.site()
Rprofile.site
mtrx <- matrix(c(3,5,8,4),nrow=2,ncol=2, byrow=TRUE)
mtrx*mtrx
ls(height)
height1 <- c(a,2,a,a,a)
ls(char="a")
ls(char<-"a")
ls("a")
ls(pat="a")
install.packages('dplyr')
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
library("dbplyr", lib.loc="~/R/win-library/3.5")
install.packages("dplyr")
install.packages("dplyr")
installed.packages()
## ----include=FALSE-------------------------------------------------------
library(tidyverse)
library(dplyr)
library(ggplot2)
install.packages('ggobi')
library(ggobi)
install.packages('gridExtra')
library(gridExtra)
install.packages('GGally')
library(GGally)
#read in birthweights data
babies <- read.table("BirthWeights.txt", header = TRUE)
glimpse(babies)
knitr::opts_chunk$set(echo = TRUE)
#install packages
install.packages('tidyverse')
install.packages('tidytext')
install.packages('textdata')
install.packages('rvest')
install.packages('topicmodels')
install.packages('wordcloud')
# Load libraries
library(tidyverse)
library(tidytext)
library(textdata)
library(rvest)
library(topicmodels)
library(wordcloud)
setwd("~/Joshua/Personal/Technical/GitHub/GoogleMaps/googlemaps-scraper")
setwd("~/Joshua/Personal/Technical/GitHub/GoogleMaps/googlemaps-scraper/data")
# Import and tidy data.
gm_reviews <- read_csv("gm_reviews.csv")
setwd("~/Joshua/Personal/Technical/GitHub/GoogleMaps/googlemaps-scraper")
# Import and tidy data.
gm_reviews <- read_csv("gm_reviews.csv")
getwd()
# Import and tidy data.
gm_reviews <- read_csv("gm_reviews.csv")
setwd("~/Joshua/Personal/Technical/GitHub/GoogleMaps/googlemaps-scraper/data")
# Import and tidy data.
#gm_reviews <- read_csv("gm_reviews.csv", file = )
gm_reviews <- read.csv("gm_reviews.csv")
# Import and tidy data.
#gm_reviews <- read_csv("gm_reviews.csv", file = )
gm_reviews <- read.csv("gm_reviews.csv")
# Import and tidy data.
#gm_reviews <- read_csv("gm_reviews.csv", file = )
gm_reviews <- read_csv("gm_reviews.csv")
# Import and tidy data.
#gm_reviews <- read_csv("gm_reviews.csv", file = )
gm_reviews <- read_csv("gm_reviews.csv")
#Q4 <- immune_expectations[,c(1,2)]
#Q19 <- immune_expectations[,c(1,3)]
gm_reviews[c(2,3,4,5,6,7)]
#Q4 <- immune_expectations[,c(1,2)]
#Q19 <- immune_expectations[,c(1,3)]
gm_reviews[c(2,3,4,5,6,7)] %>% head()
#Q4 <- immune_expectations[,c(1,2)]
#Q19 <- immune_expectations[,c(1,3)]
gm_reviews[c(2,3,4,5,6,7)] %>% head() %>% View()
clean_Q4 <- Q4 %>%
transmute(
Response = caption,
Q = "Q4",
review = Q4) %>%
unnest_tokens(word, review)
clean_Q4 <- Q4 %>%
transmute(
Response = caption,
Q = "Q4",
review = n_photo_user) %>%
unnest_tokens(word, review)
Q4 <- gm_reviews[c(2,3)]
clean_Q4 <- Q4 %>%
transmute(
Response = caption,
Q = "Q4",
review = n_photo_user) %>%
unnest_tokens(word, review)
Q4 <- gm_reviews
clean_Q4 <- Q4 %>%
transmute(
Response = caption,
Q = "Q4",
review = n_photo_user) %>%
unnest_tokens(word, review)
clean_Q4 <- Q4 %>%
unnest_tokens(word, caption)
clean_Q4
View(clean_Q4)
Q4_Count <- clean_Q4 %>%
count(word) %>%
arrange(desc(n))
Q4_Count
clean_Q4 %>%
count(word) %>%
filter(n > 2) %>%
mutate(word = fct_reorder(word, n)) %>%
ggplot(aes(x = word, y = n, fill = n)) +
geom_col(show.legend = FALSE) +
labs(title = "Word Frequency",
subtitle = "Q4 What benefits do you expect when you take immunity products?",
y = "Count",
x = "Word") +
scale_x_reordered() +
coord_flip() +
theme_classic()
clean_Q4 %>%
count(word) %>%
with(wordcloud(word, n, min.freq = 2, colors=brewer.pal(6, "Dark2")))
